{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "clear-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, length, when, col\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constants\n",
    "APP_NAME = \"LAB-2-ANTONOV-CONSUMER\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = \"-Dlog4j.configuration=file://{} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\\\n",
    "    .format(LOG4J_PROP_FILE)\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())\n",
    "\n",
    "# preparing configuration files from templates\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template\\\n",
    "    .stream(logfile=LOG_FILE)\\\n",
    "    .dump(LOG4J_PROP_FILE)\n",
    "\n",
    "#running spark\n",
    "SPARK_ADRESS = \"local[4]\"\n",
    "\n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(APP_NAME)\\\n",
    "        .master(SPARK_ADRESS)\\\n",
    "        .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "        .config(\"spark.executor.instances\", \"2\")\\\n",
    "        .config(\"spark.executor.cores\", '3')\\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "        .config(\"spark.executor.memory\", '3g')\\\n",
    "        .config(\"spark.driver.memory\", \"3g\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\\\n",
    "        .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\\\n",
    "        .config(\"spark.kubernetes.namespace\", \"aantonov-310006\")\\\n",
    "        .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "        .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"node03.st:5000/spark-executor:aantonov-310006\")\\\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", \"hdfs:///home/aantonov-310006/project\") \\\n",
    "        .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "        .config(\"spark.executor.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.path\", \"/home/jovyan/shared-data\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.path\", \"/nfs/shared\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.type\", \"Directory\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.readOnly\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "european-metadata",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Web UI: http://10.128.9.86:4041\n",
      "\n",
      "log4j file: /home/jovyan/nfs-home/LABS/LAB_KAFKA/CONS/conf/pyspark-log4j-LAB-2-ANTONOV-CONSUMER.properties\n",
      "\n",
      "driver log file: /home/jovyan/nfs-home/LABS/LAB_KAFKA/CONS/logs/pyspark-LAB-2-ANTONOV-CONSUMER.log\n"
     ]
    }
   ],
   "source": [
    "# printing important urls and pathes\n",
    "print(\"Web UI: {}\".format(spark.sparkContext.uiWebUrl))\n",
    "print(\"\\nlog4j file: {}\".format(LOG4J_PROP_FILE))\n",
    "print(\"\\ndriver log file: {}\".format(LOG_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-february",
   "metadata": {},
   "source": [
    "## Step 1\n",
    "\n",
    "Read the data sent by a producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "double-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user\", DoubleType()),\n",
    "        StructField(\"date_posted\", TimestampType()),\n",
    "        StructField(\"text\", StringType()),\n",
    "        StructField(\"sex\", StringType()),\n",
    "        StructField(\"age\", IntegerType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "middle-civilization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string, user: double, date_posted: timestamp, text: string, sex: string, age: int]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.readStream \\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka-svc:9092\")\\\n",
    "    .option(\"subscribe\", \"main\")\\\n",
    "    .load()\n",
    "\n",
    "df = df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "df = df.select('value', from_json(\"value\", schema).alias(\"value_struct\"))\n",
    "df = df.select(\"value\", \n",
    "                \"value_struct.user\", \n",
    "                \"value_struct.date_posted\", \n",
    "                \"value_struct.text\", \n",
    "                \"value_struct.sex\", \n",
    "                \"value_struct.age\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loaded-lyric",
   "metadata": {},
   "source": [
    "## Step 2\n",
    "\n",
    "Creating a word remover based on the russian stop words. UDF function get_age_group() returns and age group for a specific age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fantastic-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/home/jovyan/nltk_data\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import RegexTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aggregate-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "RS = stopwords.words('russian')\n",
    "stopwords_broadcast = spark.sparkContext.broadcast(RS) \n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"cleaned_tokens\", stopWords=RS)\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def get_age_group(age):\n",
    "    AGE_GROUPS = {(0, 18): '0-18', \n",
    "                  (18, 27): '18-27', \n",
    "                  (27, 40): '27-40', \n",
    "                  (40, 60): '40-60', \n",
    "                  (60, 500): '60+'}\n",
    "    \n",
    "    for key in AGE_GROUPS.keys():    \n",
    "        if age > key[0] and age < key[1]:\n",
    "            return AGE_GROUPS[key]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "personalized-lease",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "Dividing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "owned-success",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dividing stream by topiic: M_0_18_1_hour\n",
      "dividing stream by topiic: M_0_18_1_day\n",
      "dividing stream by topiic: M_0_18_1_week\n",
      "dividing stream by topiic: M_18_27_1_hour\n",
      "dividing stream by topiic: M_18_27_1_day\n",
      "dividing stream by topiic: M_18_27_1_week\n",
      "dividing stream by topiic: M_27_40_1_hour\n",
      "dividing stream by topiic: M_27_40_1_day\n",
      "dividing stream by topiic: M_27_40_1_week\n",
      "dividing stream by topiic: M_40_60_1_hour\n",
      "dividing stream by topiic: M_40_60_1_day\n",
      "dividing stream by topiic: M_40_60_1_week\n",
      "dividing stream by topiic: M_60_500_1_hour\n",
      "dividing stream by topiic: M_60_500_1_day\n",
      "dividing stream by topiic: M_60_500_1_week\n",
      "dividing stream by topiic: F_0_18_1_hour\n",
      "dividing stream by topiic: F_0_18_1_day\n",
      "dividing stream by topiic: F_0_18_1_week\n",
      "dividing stream by topiic: F_18_27_1_hour\n",
      "dividing stream by topiic: F_18_27_1_day\n",
      "dividing stream by topiic: F_18_27_1_week\n",
      "dividing stream by topiic: F_27_40_1_hour\n",
      "dividing stream by topiic: F_27_40_1_day\n",
      "dividing stream by topiic: F_27_40_1_week\n",
      "dividing stream by topiic: F_40_60_1_hour\n",
      "dividing stream by topiic: F_40_60_1_day\n",
      "dividing stream by topiic: F_40_60_1_week\n",
      "dividing stream by topiic: F_60_500_1_hour\n",
      "dividing stream by topiic: F_60_500_1_day\n",
      "dividing stream by topiic: F_60_500_1_week\n"
     ]
    }
   ],
   "source": [
    "SEX_GROUPS = [\"M\", \"F\"]\n",
    "AGE_GROUPS = [(0, 18), (18, 27), (27, 40), (40, 60), (60, 500)]\n",
    "DURATION_GROPUS = ['1 hour', '1 day', '1 week']\n",
    "\n",
    "for sex in SEX_GROUPS:\n",
    "    for age_bounds in AGE_GROUPS:\n",
    "        age_lower_bound = age_bounds[0]\n",
    "        age_upper_bound = age_bounds[1]\n",
    "        for dur in DURATION_GROPUS:\n",
    "            \n",
    "            dur_normalised = dur.replace(\" \", \"_\")\n",
    "            topic = f\"{sex}_{age_lower_bound}_{age_upper_bound}_{dur_normalised}\"\n",
    "            print(f\"dividing stream by topiic: {topic}\")\n",
    "\n",
    "            remover.transform(\n",
    "                df\\\n",
    "                .withColumn(\"tokens\", split(col('text'), ' '))\\\n",
    "                .withColumn(\"age_group\", get_age_group(\"age\"))\n",
    "            )\\\n",
    "            .where((col(\"sex\") == sex) & (col(\"age\") > age_lower_bound) & (col(\"age\") < age_upper_bound))\\\n",
    "            .select(\n",
    "                \"date_posted\", \n",
    "                \"age_group\",\n",
    "                \"sex\",\n",
    "                explode(\"cleaned_tokens\").alias(\"token\")\n",
    "            )\\\n",
    "            .groupBy('sex', 'age_group', window('date_posted', dur, dur))\\\n",
    "            .agg(count('token').alias('tokens_count'))\\\n",
    "            .selectExpr('to_json(struct(*)) as value')\\\n",
    "            .writeStream\\\n",
    "            .outputMode(\"update\")\\\n",
    "            .format(\"kafka\") \\\n",
    "            .option(\"kafka.bootstrap.servers\", \"kafka-svc:9092\")\\\n",
    "            .option(\"topic\", topic)\\\n",
    "            .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-energy",
   "metadata": {},
   "source": [
    "Lets create a test consumer to look at the topic \"female in age group 18-27 windowed by week\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ultimate-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(bootstrap_servers=\"kafka-svc:9092\", consumer_timeout_ms=1000)\n",
    "consumer.subscribe('F_18_27_1_week')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-china",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sex': 'F', 'age_group': '18-27', 'window': {'start': '2018-12-27T00:00:00.000Z', 'end': '2019-01-03T00:00:00.000Z'}, 'tokens_count': 78}\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    for message in consumer:\n",
    "        print(json.loads(message.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-browse",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-eclipse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
