{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import socket\n",
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, length, when, col\n",
    "from pyspark.sql.types import BooleanType, IntegerType, LongType, StringType, ArrayType, FloatType, StructType, StructField, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "from pyspark.sql.functions import PandasUDFType\n",
    "from jinja2 import Environment, FileSystemLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting constants\n",
    "APP_NAME = \"VVORONIN-SPARK-APP_receive\"\n",
    "NORMALIZED_APP_NAME = APP_NAME.replace('/', '_').replace(':', '_')\n",
    "\n",
    "APPS_TMP_DIR = os.path.join(os.getcwd(), \"tmp\")\n",
    "APPS_CONF_DIR = os.path.join(os.getcwd(), \"conf\")\n",
    "APPS_LOGS_DIR = os.path.join(os.getcwd(), \"logs\")\n",
    "LOG4J_PROP_FILE = os.path.join(APPS_CONF_DIR, \"pyspark-log4j-{}.properties\".format(NORMALIZED_APP_NAME))\n",
    "LOG_FILE = os.path.join(APPS_LOGS_DIR, 'pyspark-{}.log'.format(NORMALIZED_APP_NAME))\n",
    "EXTRA_JAVA_OPTIONS = \"-Dlog4j.configuration=file://{} -Dspark.hadoop.dfs.replication=1 -Dhttps.protocols=TLSv1.0,TLSv1.1,TLSv1.2,TLSv1.3\"\\\n",
    "    .format(LOG4J_PROP_FILE)\n",
    "\n",
    "LOCAL_IP = socket.gethostbyname(socket.gethostname())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing configuration files from templates\n",
    "for directory in [APPS_CONF_DIR, APPS_LOGS_DIR, APPS_TMP_DIR]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "env = Environment(loader=FileSystemLoader('/opt'))\n",
    "template = env.get_template(\"pyspark_log4j.properties.template\")\n",
    "template\\\n",
    "    .stream(logfile=LOG_FILE)\\\n",
    "    .dump(LOG4J_PROP_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPARK_ADDRESS = 'local[4]'\n",
    "# run spark\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(APP_NAME)\\\n",
    "        .master(SPARK_ADDRESS)\\\n",
    "        .config(\"spark.driver.host\", LOCAL_IP)\\\n",
    "        .config(\"spark.driver.bindAddress\", \"0.0.0.0\")\\\n",
    "        .config(\"spark.executor.instances\", \"2\")\\\n",
    "        .config(\"spark.executor.cores\", '3')\\\n",
    "        .config(\"spark.memory.fraction\", \"0.8\")\\\n",
    "        .config(\"spark.memory.storageFraction\", \"0.6\")\\\n",
    "        .config(\"spark.executor.memory\", '3g')\\\n",
    "        .config(\"spark.driver.memory\", \"3g\")\\\n",
    "        .config(\"spark.driver.maxResultSize\", \"1g\")\\\n",
    "        .config(\"spark.kubernetes.memoryOverheadFactor\", \"0.3\")\\\n",
    "        .config(\"spark.driver.extraJavaOptions\", EXTRA_JAVA_OPTIONS)\\\n",
    "        .config(\"spark.kubernetes.namespace\", \"vvoronin-306285\")\\\n",
    "        .config(\"spark.kubernetes.driver.label.appname\", APP_NAME)\\\n",
    "        .config(\"spark.kubernetes.executor.label.appname\", APP_NAME)\\\n",
    "        .config(\"spark.kubernetes.container.image\", \"node03.st:5000/spark-executor:vvoronin-306285\")\\\n",
    "        .config(\"spark.sql.streaming.checkpointLocation\", \"hdfs:///home/vvoronin-306285\") \\\n",
    "        .config(\"spark.local.dir\", \"/tmp/spark\")\\\n",
    "        .config(\"spark.driver.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "        .config(\"spark.executor.extraClassPath\", \"/home/jovyan/shared-data/my-project-name-jar-with-dependencies.jar\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.path\", \"/tmp/spark\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.emptyDir.spark-local-dir-tmp-spark.mount.readOnly\", \"false\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.path\", \"/home/jovyan/shared-data\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.path\", \"/nfs/shared\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.options.type\", \"Directory\")\\\n",
    "        .config(\"spark.kubernetes.executor.volumes.hostPath.depdir.mount.readOnly\", \"true\")\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "        StructField(\"user_id\", DoubleType()),\n",
    "        StructField(\"date\", TimestampType()),\n",
    "        StructField(\"text\", StringType()),\n",
    "        StructField(\"sex\", StringType()),\n",
    "        StructField(\"age\", IntegerType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = spark.readStream \\\n",
    "                .format(\"kafka\")\\\n",
    "                .option(\"kafka.bootstrap.servers\", \"kafka-svc:9092\")\\\n",
    "                .option(\"subscribe\", \"main\")\\\n",
    "                .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream_df.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = stream_df.select('value', from_json(\"value\", schema).alias(\"value_struct\"))\n",
    "stream_df = stream_df.select(\"value\", \n",
    "                \"value_struct.user_id\", \n",
    "                \"value_struct.date\", \n",
    "                \"value_struct.text\", \n",
    "                \"value_struct.sex\", \n",
    "                \"value_struct.age\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удаляем стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.data.path.append(\"/home/jovyan/nltk_data\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pyspark.ml.feature import RegexTokenizer\n",
    "from pyspark.ml.feature import StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_ru = stopwords.words('russian')\n",
    "stopwords_broadcast = spark.sparkContext.broadcast(stop_words_ru) \n",
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"cleaned_tokens\", stopWords=stop_words_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_categories(age):\n",
    "    categories = {(0, 18): '0-18', \n",
    "                  (18, 27): '18-27', \n",
    "                  (27, 40): '27-40', \n",
    "                  (40, 60): '40-60', \n",
    "                  (60, 130): '60+'}\n",
    "    \n",
    "    for key in categories.keys():    \n",
    "        if age > key[0] and age < key[1]:\n",
    "            return categories[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dividing topic: M_0_18_hour\n",
      "Dividing topic: M_0_18_day\n",
      "Dividing topic: M_0_18_week\n",
      "Dividing topic: M_18_27_hour\n",
      "Dividing topic: M_18_27_day\n",
      "Dividing topic: M_18_27_week\n",
      "Dividing topic: M_27_40_hour\n",
      "Dividing topic: M_27_40_day\n",
      "Dividing topic: M_27_40_week\n",
      "Dividing topic: M_40_60_hour\n",
      "Dividing topic: M_40_60_day\n",
      "Dividing topic: M_40_60_week\n",
      "Dividing topic: M_60_500_hour\n",
      "Dividing topic: M_60_500_day\n",
      "Dividing topic: M_60_500_week\n",
      "Dividing topic: F_0_18_hour\n",
      "Dividing topic: F_0_18_day\n",
      "Dividing topic: F_0_18_week\n",
      "Dividing topic: F_18_27_hour\n",
      "Dividing topic: F_18_27_day\n",
      "Dividing topic: F_18_27_week\n",
      "Dividing topic: F_27_40_hour\n",
      "Dividing topic: F_27_40_day\n",
      "Dividing topic: F_27_40_week\n",
      "Dividing topic: F_40_60_hour\n",
      "Dividing topic: F_40_60_day\n",
      "Dividing topic: F_40_60_week\n",
      "Dividing topic: F_60_500_hour\n",
      "Dividing topic: F_60_500_day\n",
      "Dividing topic: F_60_500_week\n"
     ]
    }
   ],
   "source": [
    "sex_list = [\"M\", \"F\"]\n",
    "categories = [(0, 18), (18, 27), (27, 40), (40, 60), (60, 500)]\n",
    "periods = ['hour', 'day', 'week']\n",
    "\n",
    "for sex in sex_list:\n",
    "    for age in categories:\n",
    "        age_lower = age[0]\n",
    "        age_upper = age[1]\n",
    "        for period in periods:\n",
    "            #period_norm = period.replace(\" \", \"_\")\n",
    "            topic = f\"{sex}_{age_lower}_{age_upper}_{period}\"\n",
    "            print(f\"Dividing topic: {topic}\")\n",
    "\n",
    "            remover.transform(stream_df.withColumn(\"tokens\", split(col('text'), ' '))\\\n",
    "                                .withColumn(\"age_group\", get_categories(\"age\")))\\\n",
    "                                .where((col(\"sex\") == sex) & (col(\"age\") > age_lower) & (col(\"age\") < age_upper))\\\n",
    "                                .select(\"date\",\"age_group\",\"sex\", explode(\"cleaned_tokens\").alias(\"token\"))\\\n",
    "                                .groupBy('sex', 'age_group', window('date', period, period))\\\n",
    "                                .agg(count('token').alias('tokens_count'))\\\n",
    "                                .selectExpr('to_json(struct(*)) as value')\\\n",
    "                                .writeStream\\\n",
    "                                .outputMode(\"update\")\\\n",
    "                                .format(\"kafka\") \\\n",
    "                                .option(\"kafka.bootstrap.servers\", \"kafka-svc:9092\")\\\n",
    "                                .option(\"topic\", topic)\\\n",
    "                                .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaConsumer\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = KafkaConsumer(bootstrap_servers=\"kafka-svc:9092\", consumer_timeout_ms=1000)\n",
    "consumer.subscribe('M_18_27_day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    for message in consumer:\n",
    "        print(json.loads(message.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
